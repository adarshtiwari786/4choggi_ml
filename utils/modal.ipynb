{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d5ee197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284a5c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# 1) Setup\n",
    "import os, re, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pdfplumber, fitz, requests\n",
    "\n",
    "# %%\n",
    "# 2) Config\n",
    "DATA_DIR = Path(\"./data\")\n",
    "JD_FILE = DATA_DIR / \"job_description.pdf\"\n",
    "RESUMES_DIR = DATA_DIR / \"resumes\"\n",
    "GITHUB_INSIGHTS_FILE = DATA_DIR / \"github_insights.json\"  # sample file with insights\n",
    "\n",
    "OPENROUTER_API_KEY = \"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\"\n",
    "SENTENCE_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "THRESHOLD = 0.4 \n",
    "\n",
    "# %%\n",
    "# 3) PDF extraction\n",
    "def extract_text_from_pdf(filepath):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(filepath) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() or \"\"\n",
    "    except:\n",
    "        try:\n",
    "            with fitz.open(filepath) as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filepath}: {e}\")\n",
    "    return re.sub(r'\\s+', ' ', text.strip())\n",
    "\n",
    "# %%\n",
    "# 4) Load JD, resumes, and GitHub insights\n",
    "def load_documents():\n",
    "    jd_text = extract_text_from_pdf(JD_FILE)\n",
    "    resumes = {f.name: extract_text_from_pdf(f) for f in RESUMES_DIR.glob('*.pdf')}\n",
    "    github_insights = {}\n",
    "    if GITHUB_INSIGHTS_FILE.exists():2\n",
    "        with open(GITHUB_INSIGHTS_FILE, 'r') as f:\n",
    "            github_insights = json.load(f)\n",
    "    return jd_text, resumes, github_insights\n",
    "\n",
    "# %%\n",
    "# 5) Extract keywords (optional helper)\n",
    "def extract_keywords(text):\n",
    "    words = re.findall(r'\\b[a-zA-Z]{3,}\\b', text.lower())\n",
    "    stop = {\"the\",\"and\",\"for\",\"with\",\"this\",\"that\",\"from\",\"your\",\"have\",\"are\",\"was\",\"you\"}\n",
    "    words = [w for w in words if w not in stop]\n",
    "    return list(set(words))\n",
    "\n",
    "# %%\n",
    "# 6) Compute similarity\n",
    "def compute_similarity(jd_text, resumes, github_insights):\n",
    "    model = SentenceTransformer(SENTENCE_MODEL_NAME)\n",
    "    jd_emb = model.encode([jd_text])\n",
    "    results = []\n",
    "\n",
    "    print(\"\\nðŸ” Calculating Similarities...\\n\")\n",
    "    for name, text in resumes.items():\n",
    "        github_text = github_insights.get(name.replace('.pdf',''), '')\n",
    "        combined = text + ' ' + github_text\n",
    "\n",
    "        if len(combined.strip()) == 0:\n",
    "            print(f\"âš ï¸ Skipping empty file: {name}\")\n",
    "            continue\n",
    "\n",
    "        emb = model.encode([combined])\n",
    "        sim = cosine_similarity(jd_emb, emb)[0][0]\n",
    "        print(f\"{name}: {sim:.3f}\")  # diagnostic print\n",
    "\n",
    "        if sim >= THRESHOLD:\n",
    "            results.append((name, sim))\n",
    "\n",
    "    df = pd.DataFrame(results, columns=['Candidate', 'Similarity'])\n",
    "    df = df.sort_values('Similarity', ascending=False)\n",
    "    return df\n",
    "\n",
    "# %%\n",
    "# 7) Run pipeline\n",
    "jd_text, resumes, github_insights = load_documents()\n",
    "\n",
    "print(\"JD text length:\", len(jd_text))\n",
    "for name, text in resumes.items():\n",
    "    print(f\"{name} â†’ {len(text)} characters\")\n",
    "\n",
    "df = compute_similarity(jd_text, resumes, github_insights)\n",
    "print(\"\\nAll Results:\\n\", df)\n",
    "\n",
    "# %%\n",
    "# 8) Filtered Results (>= 0.6)\n",
    "filtered = df[df['Similarity'] >= THRESHOLD]\n",
    "print(\"\\nTop Matches (>= 0.6):\\n\", filtered)\n",
    "\n",
    "# %%\n",
    "# 9) Save to CSV\n",
    "filtered.to_csv('filtered_matches.csv', index=False)\n",
    "print(\"\\nâœ… Results saved to filtered_matches.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
